{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e631ac6f-dfc3-4873-aa39-b347acbef7c3",
   "metadata": {},
   "source": [
    "# Baseline Transfer Learning Model for TrashNet Classification\n",
    "Our baseline model will include a pretrained DenseNet feature extractor with a shallow and wide CNN head. This model will have a homogenous learning rate. We are going to use K-Fold CV as well as F1 score and multi-class AUC to validate our model.\n",
    "This model acts as a stepping stone / template for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5a3ed2-cc71-42cf-afbb-9d3e675e3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pkbar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330641b6-88c9-4b57-ae4f-10bbbbbc45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT = Path('../asun/Smart-Trash/data/isbnet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d609d1-99da-4d2b-a8a0-2ec86d27587b",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "For the baseline model, we will not be applying any data augmentation or color manipulation.\n",
    "- Get the index CSV file that includes all files their respective directory and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe7c025-aea7-4ebe-a4da-20c5a652a5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2990f3-ecec-4557-b90b-616f630dbe34",
   "metadata": {},
   "source": [
    "### Trash Dataset\n",
    "Dataset object to handle various sets of data that we will be dealing with including: TrashNet, ISBNet, and ISBNet extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bf678a-c6e9-4e1b-a023-e8572269297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(length, split, shuffle=True):\n",
    "    \"\"\"\n",
    "    :returns: random samplers for both the training dataset and the validation dataset.\n",
    "    \"\"\"\n",
    "    indicies = list(range(length))\n",
    "    split = int(np.floor(split * length))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indicies)\n",
    "    \n",
    "    train_indicies, val_indicies = indicies[split:], indicies[:split]\n",
    "    train_sampler = data.SubsetRandomSampler(train_indicies)\n",
    "    val_sampler = data.SubsetRandomSampler(val_indicies)\n",
    "    \n",
    "    return train_sampler, val_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a27a2d-a964-4ac8-96b1-11ccafba101d",
   "metadata": {},
   "source": [
    "A split function is defined to split the dataset after it is defined as a `DataSet` object. This makes it really easy to handle, because after splitting it, we are creating a sampler object. There is no need to modify the dataset object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8867e75-19f9-40df-9b36-4aab8689dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashDataset(Dataset):\n",
    "    def __init__(self, metadata: pd.DataFrame, directory: Path, transform=None):\n",
    "        \"\"\"\n",
    "        metadata: DataFrame that contains information about each image and their labels.\n",
    "        directory: the directory where the trash data is kept\n",
    "        root_dir: path to the `directory`\n",
    "        transform: optional augmentations that are to be applied onto the images\n",
    "        \"\"\"\n",
    "        self.images_folder = directory\n",
    "        self.meta = metadata\n",
    "        self.transform = transform\n",
    "        self.label_dict = {\n",
    "            'cans': 0,\n",
    "            'landfill': 1,\n",
    "            'paper': 2,\n",
    "            'plastic': 3,\n",
    "            'tetrapak': 4\n",
    "        }\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = Image.open(self.meta.iloc[idx, 0])\n",
    "        labels = [0] * 5\n",
    "        labels[self.label_dict[self.meta.iloc[idx, 1]]] = 1\n",
    "        sample = {'image': image,\n",
    "                  'path': self.meta.iloc[idx, 0],\n",
    "                  'label': torch.tensor(self.label_dict[self.meta.iloc[idx, 1]], dtype=torch.float)}\n",
    "\n",
    "        if self.transform:\n",
    "              sample['image'] = self.transform(sample['image'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038302ee-e37b-48c2-a214-b3eba8740a8a",
   "metadata": {},
   "source": [
    "Because we are using cross entropy loss we can express the loss as simply a scalar. This scalar is between [0-numclasses]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe16fc-069c-4c72-96f3-dccc85f1abc8",
   "metadata": {},
   "source": [
    "## Model and Training Setup\n",
    "- VGG16 pretrained with ImageNet\n",
    "- Wide and shallow CNN with fully connected and log-softmax activation\n",
    "- CrossEntropy loss and Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e3a55-a797-4ba3-be53-50b3e8fa4e33",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1903e9af-70b0-4b46-b89f-4ca8c86902e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16BN, self).__init__()\n",
    "        self.head = models.densenet169(pretrained=True)\n",
    "        self.head.requires_grad = False\n",
    "        # Remove classification layers so that we are able to add our own CNN layers\n",
    "        self.head.classifier = nn.Sequential(\n",
    "                                    nn.Linear(1664, 1024, bias=True),\n",
    "                                    nn.BatchNorm1d(1024),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.05),\n",
    "                                    nn.Linear(1024, 512, bias=True),\n",
    "                                    nn.BatchNorm1d(512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.10),\n",
    "                                    nn.Linear(512, 5, bias=True),\n",
    "                                    nn.ReLU())\n",
    "    def freeze(self, n_top, freeze_head=True):\n",
    "        \"\"\"\n",
    "        Freeze head layers.\n",
    "        \"\"\"\n",
    "        for index, layer in enumerate(self.head.children()):\n",
    "            if index == len(list(self.head.children())) - 1:\n",
    "                return\n",
    "            else:\n",
    "                layer.requires_grad = False\n",
    "#         self.head.features.requires_grad = not freeze_head\n",
    "#         for head in range(n_top):\n",
    "#             self.head.classifier[head].requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "  \n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"\n",
    "        https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "        \"\"\"\n",
    "        size = x.size()[1:]  # get all dimensions except for batch size\n",
    "        features = 1\n",
    "        for s in size:\n",
    "            features *= s\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524748a5-c23f-4cfe-9be6-00969520d023",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7dfa35c-fe00-421f-8f60-923e42aa425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abfe31-c247-40c3-87ea-0de9776f7ec4",
   "metadata": {},
   "source": [
    "### KFold Training and CV\n",
    "* KFold setup with `StratifiedKFold`\n",
    "* Creating Dataloaders in training loop.\n",
    "* Using Adam and CrossEntropy Loss\n",
    "* Center crop on images to make them 224x224 so VGG will be able to take them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b62ff5-0f0a-465b-912d-2022d1b6e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d26543c-22fe-4287-af82-c1240bee0508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>category</th>\n",
       "      <th>categorial_time</th>\n",
       "      <th>trashcan_id</th>\n",
       "      <th>trashcan_time</th>\n",
       "      <th>trashcan_location</th>\n",
       "      <th>landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/cans/9A/202001...</td>\n",
       "      <td>cans</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 0, 0, 0, 0)</td>\n",
       "      <td>9A</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 1, 1, 1)</td>\n",
       "      <td>[(24.07, 7.791), (2.209, 7.907), (8.372, 6.047...</td>\n",
       "      <td>['theater', 'printer', 'stairwell', 'bathroom'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/cans/9A/202001...</td>\n",
       "      <td>cans</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 0, 0, 0, 0)</td>\n",
       "      <td>9A</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 1, 1, 1)</td>\n",
       "      <td>[(24.07, 7.791), (2.209, 7.907), (8.372, 6.047...</td>\n",
       "      <td>['theater', 'printer', 'stairwell', 'bathroom'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/cans/9A/202001...</td>\n",
       "      <td>cans</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 0, 0, 0, 0)</td>\n",
       "      <td>9A</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 1, 1, 1)</td>\n",
       "      <td>[(24.07, 7.791), (2.209, 7.907), (8.372, 6.047...</td>\n",
       "      <td>['theater', 'printer', 'stairwell', 'bathroom'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/cans/6B/cans29...</td>\n",
       "      <td>cans</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 0, 0, 0, 0)</td>\n",
       "      <td>6B</td>\n",
       "      <td>(0, 1, 0, 1, 0, 0, 1, 0, 0, 0)</td>\n",
       "      <td>[(0.465, 7.442), (15.116, 6.744), (10.349, 7.5...</td>\n",
       "      <td>['bathroom', 'stairwell', 'couch_area']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/cans/6B/IMG_73...</td>\n",
       "      <td>cans</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 0, 0, 0, 0)</td>\n",
       "      <td>6B</td>\n",
       "      <td>(0, 1, 0, 1, 0, 0, 1, 0, 0, 0)</td>\n",
       "      <td>[(0.465, 7.442), (15.116, 6.744), (10.349, 7.5...</td>\n",
       "      <td>['bathroom', 'stairwell', 'couch_area']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/tetrapak/8A/tr...</td>\n",
       "      <td>tetrapak</td>\n",
       "      <td>(0, 1, 1, 1, 1, 1, 0, 1, 1, 0)</td>\n",
       "      <td>8A</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 1, 1, 0, 0)</td>\n",
       "      <td>[(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...</td>\n",
       "      <td>['cafeteria', 'stairwell', 'bathroom', 'librar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/tetrapak/8A/tr...</td>\n",
       "      <td>tetrapak</td>\n",
       "      <td>(0, 1, 1, 1, 1, 1, 0, 1, 1, 0)</td>\n",
       "      <td>8A</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 1, 1, 0, 0)</td>\n",
       "      <td>[(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...</td>\n",
       "      <td>['cafeteria', 'stairwell', 'bathroom', 'librar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/tetrapak/8A/20...</td>\n",
       "      <td>tetrapak</td>\n",
       "      <td>(0, 1, 1, 1, 1, 1, 0, 1, 1, 0)</td>\n",
       "      <td>8A</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 1, 1, 0, 0)</td>\n",
       "      <td>[(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...</td>\n",
       "      <td>['cafeteria', 'stairwell', 'bathroom', 'librar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/tetrapak/8A/tr...</td>\n",
       "      <td>tetrapak</td>\n",
       "      <td>(0, 1, 1, 1, 1, 1, 0, 1, 1, 0)</td>\n",
       "      <td>8A</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 1, 1, 0, 0)</td>\n",
       "      <td>[(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...</td>\n",
       "      <td>['cafeteria', 'stairwell', 'bathroom', 'librar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>../asun/Smart-Trash/data/isbnet/tetrapak/8A/20...</td>\n",
       "      <td>tetrapak</td>\n",
       "      <td>(0, 1, 1, 1, 1, 1, 0, 1, 1, 0)</td>\n",
       "      <td>8A</td>\n",
       "      <td>(0, 0, 0, 1, 1, 1, 1, 1, 0, 0)</td>\n",
       "      <td>[(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...</td>\n",
       "      <td>['cafeteria', 'stairwell', 'bathroom', 'librar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>888 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filepath  category  \\\n",
       "0    ../asun/Smart-Trash/data/isbnet/cans/9A/202001...      cans   \n",
       "1    ../asun/Smart-Trash/data/isbnet/cans/9A/202001...      cans   \n",
       "2    ../asun/Smart-Trash/data/isbnet/cans/9A/202001...      cans   \n",
       "3    ../asun/Smart-Trash/data/isbnet/cans/6B/cans29...      cans   \n",
       "4    ../asun/Smart-Trash/data/isbnet/cans/6B/IMG_73...      cans   \n",
       "..                                                 ...       ...   \n",
       "883  ../asun/Smart-Trash/data/isbnet/tetrapak/8A/tr...  tetrapak   \n",
       "884  ../asun/Smart-Trash/data/isbnet/tetrapak/8A/tr...  tetrapak   \n",
       "885  ../asun/Smart-Trash/data/isbnet/tetrapak/8A/20...  tetrapak   \n",
       "886  ../asun/Smart-Trash/data/isbnet/tetrapak/8A/tr...  tetrapak   \n",
       "887  ../asun/Smart-Trash/data/isbnet/tetrapak/8A/20...  tetrapak   \n",
       "\n",
       "                    categorial_time trashcan_id  \\\n",
       "0    (0, 0, 0, 1, 1, 1, 0, 0, 0, 0)          9A   \n",
       "1    (0, 0, 0, 1, 1, 1, 0, 0, 0, 0)          9A   \n",
       "2    (0, 0, 0, 1, 1, 1, 0, 0, 0, 0)          9A   \n",
       "3    (0, 0, 0, 1, 1, 1, 0, 0, 0, 0)          6B   \n",
       "4    (0, 0, 0, 1, 1, 1, 0, 0, 0, 0)          6B   \n",
       "..                              ...         ...   \n",
       "883  (0, 1, 1, 1, 1, 1, 0, 1, 1, 0)          8A   \n",
       "884  (0, 1, 1, 1, 1, 1, 0, 1, 1, 0)          8A   \n",
       "885  (0, 1, 1, 1, 1, 1, 0, 1, 1, 0)          8A   \n",
       "886  (0, 1, 1, 1, 1, 1, 0, 1, 1, 0)          8A   \n",
       "887  (0, 1, 1, 1, 1, 1, 0, 1, 1, 0)          8A   \n",
       "\n",
       "                      trashcan_time  \\\n",
       "0    (0, 0, 0, 0, 0, 0, 0, 1, 1, 1)   \n",
       "1    (0, 0, 0, 0, 0, 0, 0, 1, 1, 1)   \n",
       "2    (0, 0, 0, 0, 0, 0, 0, 1, 1, 1)   \n",
       "3    (0, 1, 0, 1, 0, 0, 1, 0, 0, 0)   \n",
       "4    (0, 1, 0, 1, 0, 0, 1, 0, 0, 0)   \n",
       "..                              ...   \n",
       "883  (0, 0, 0, 1, 1, 1, 1, 1, 0, 0)   \n",
       "884  (0, 0, 0, 1, 1, 1, 1, 1, 0, 0)   \n",
       "885  (0, 0, 0, 1, 1, 1, 1, 1, 0, 0)   \n",
       "886  (0, 0, 0, 1, 1, 1, 1, 1, 0, 0)   \n",
       "887  (0, 0, 0, 1, 1, 1, 1, 1, 0, 0)   \n",
       "\n",
       "                                     trashcan_location  \\\n",
       "0    [(24.07, 7.791), (2.209, 7.907), (8.372, 6.047...   \n",
       "1    [(24.07, 7.791), (2.209, 7.907), (8.372, 6.047...   \n",
       "2    [(24.07, 7.791), (2.209, 7.907), (8.372, 6.047...   \n",
       "3    [(0.465, 7.442), (15.116, 6.744), (10.349, 7.5...   \n",
       "4    [(0.465, 7.442), (15.116, 6.744), (10.349, 7.5...   \n",
       "..                                                 ...   \n",
       "883  [(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...   \n",
       "884  [(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...   \n",
       "885  [(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...   \n",
       "886  [(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...   \n",
       "887  [(17.558, 31.977), (7.558, 1.86), (9.186, 13.8...   \n",
       "\n",
       "                                             landmarks  \n",
       "0    ['theater', 'printer', 'stairwell', 'bathroom'...  \n",
       "1    ['theater', 'printer', 'stairwell', 'bathroom'...  \n",
       "2    ['theater', 'printer', 'stairwell', 'bathroom'...  \n",
       "3              ['bathroom', 'stairwell', 'couch_area']  \n",
       "4              ['bathroom', 'stairwell', 'couch_area']  \n",
       "..                                                 ...  \n",
       "883  ['cafeteria', 'stairwell', 'bathroom', 'librar...  \n",
       "884  ['cafeteria', 'stairwell', 'bathroom', 'librar...  \n",
       "885  ['cafeteria', 'stairwell', 'bathroom', 'librar...  \n",
       "886  ['cafeteria', 'stairwell', 'bathroom', 'librar...  \n",
       "887  ['cafeteria', 'stairwell', 'bathroom', 'librar...  \n",
       "\n",
       "[888 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(ROOT / 'metadata.csv')\n",
    "TRAIN_VAL = TrashDataset(metadata, ROOT, transform)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47534dd-2b78-4268-8ba1-28e83c0cd5f7",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9c5227-6252-4082-ab54-16c9a41459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VGG16BN()\n",
    "#model.freeze(5)\n",
    "# model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "#model = model.to(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b07b0f-1d59-4ba8-adb2-0f7f6e400543",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186aa974-882c-4f43-85f8-678c2162d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#celoss = nn.CrossEntropyLoss(weight=torch.tensor([6.0241, 3.6496, 2.4390, 1.0823, 4.3860]).to(0, dtype=torch.float))\n",
    "#optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ad92c-9bf9-4e36-aa8b-6d6d625750b6",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0475459-6088-43d9-9476-75519ae807b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sampler, val_sampler = split(len(TRAIN_VAL), 0.13)\n",
    "#train_loader = DataLoader(TRAIN_VAL, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=4)\n",
    "#valid_loader = DataLoader(TRAIN_VAL, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=4)\n",
    "\n",
    "trash_labels = metadata.iloc[:,1].values\n",
    "#print(trash_labels)\n",
    "\n",
    "s = StratifiedKFold(n_splits=FOLDS, shuffle=True).split(metadata, trash_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b8537-a74b-4cf2-85d2-748f375dcc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Epoch: 1/50 Max F1: 0\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 1.1509 - F1_Score: 0.4459 - Accuracy: 0.5459 - val_CELoss: 0.9965 - val_F1_Score: 0.5321 - val_Accuracy: 0.6372\n",
      "Fold: 1 Epoch: 2/50 Max F1: 0.5320718606012724\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.6515 - F1_Score: 0.6511 - Accuracy: 0.7306 - val_CELoss: 0.5679 - val_F1_Score: 0.7368 - val_Accuracy: 0.7841\n",
      "Fold: 1 Epoch: 3/50 Max F1: 0.7368379311320488\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.4486 - F1_Score: 0.7886 - Accuracy: 0.8423 - val_CELoss: 0.3940 - val_F1_Score: 0.8103 - val_Accuracy: 0.8536\n",
      "Fold: 1 Epoch: 4/50 Max F1: 0.8102994814674114\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.3118 - F1_Score: 0.8897 - Accuracy: 0.9091 - val_CELoss: 0.3985 - val_F1_Score: 0.8353 - val_Accuracy: 0.8628\n",
      "Fold: 1 Epoch: 5/50 Max F1: 0.835318701360368\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.2700 - F1_Score: 0.8830 - Accuracy: 0.9091 - val_CELoss: 0.4161 - val_F1_Score: 0.8407 - val_Accuracy: 0.8420\n",
      "Fold: 1 Epoch: 6/50 Max F1: 0.8407265995994155\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.2205 - F1_Score: 0.9123 - Accuracy: 0.9313 - val_CELoss: 0.4022 - val_F1_Score: 0.8469 - val_Accuracy: 0.8513\n",
      "Fold: 1 Epoch: 7/50 Max F1: 0.8469244661318699\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.2215 - F1_Score: 0.8925 - Accuracy: 0.9186 - val_CELoss: 0.3587 - val_F1_Score: 0.8900 - val_Accuracy: 0.8866\n",
      "Fold: 1 Epoch: 8/50 Max F1: 0.889987671346367\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.1998 - F1_Score: 0.8939 - Accuracy: 0.9190 - val_CELoss: 0.3241 - val_F1_Score: 0.8984 - val_Accuracy: 0.8970\n",
      "Fold: 1 Epoch: 9/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.1847 - F1_Score: 0.9158 - Accuracy: 0.9332 - val_CELoss: 0.3431 - val_F1_Score: 0.8663 - val_Accuracy: 0.8709\n",
      "Fold: 1 Epoch: 10/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.2041 - F1_Score: 0.9195 - Accuracy: 0.9280 - val_CELoss: 0.2739 - val_F1_Score: 0.8301 - val_Accuracy: 0.9062\n",
      "Fold: 1 Epoch: 11/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.1905 - F1_Score: 0.9137 - Accuracy: 0.9299 - val_CELoss: 0.2868 - val_F1_Score: 0.8972 - val_Accuracy: 0.9062\n",
      "Fold: 1 Epoch: 12/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.2125 - F1_Score: 0.9032 - Accuracy: 0.9124 - val_CELoss: 0.2558 - val_F1_Score: 0.8327 - val_Accuracy: 0.8837\n",
      "Fold: 1 Epoch: 13/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 186s 8s/step - CELoss: 0.2100 - F1_Score: 0.8846 - Accuracy: 0.9214 - val_CELoss: 0.3391 - val_F1_Score: 0.8585 - val_Accuracy: 0.9005\n",
      "Fold: 1 Epoch: 14/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 186s 8s/step - CELoss: 0.1776 - F1_Score: 0.9030 - Accuracy: 0.9309 - val_CELoss: 0.3754 - val_F1_Score: 0.8903 - val_Accuracy: 0.8877\n",
      "Fold: 1 Epoch: 15/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.2163 - F1_Score: 0.8762 - Accuracy: 0.9119 - val_CELoss: 0.3479 - val_F1_Score: 0.8670 - val_Accuracy: 0.8709\n",
      "Fold: 1 Epoch: 16/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.2242 - F1_Score: 0.8980 - Accuracy: 0.9124 - val_CELoss: 0.3747 - val_F1_Score: 0.8861 - val_Accuracy: 0.8889\n",
      "Fold: 1 Epoch: 17/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.2055 - F1_Score: 0.9041 - Accuracy: 0.9257 - val_CELoss: 0.3419 - val_F1_Score: 0.8732 - val_Accuracy: 0.8762\n",
      "Fold: 1 Epoch: 18/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.2110 - F1_Score: 0.8916 - Accuracy: 0.9271 - val_CELoss: 0.2929 - val_F1_Score: 0.8864 - val_Accuracy: 0.8900\n",
      "Fold: 1 Epoch: 19/50 Max F1: 0.8983689472883847\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.1555 - F1_Score: 0.9111 - Accuracy: 0.9413 - val_CELoss: 0.2425 - val_F1_Score: 0.9002 - val_Accuracy: 0.9282\n",
      "Fold: 1 Epoch: 20/50 Max F1: 0.9002408344852587\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.0993 - F1_Score: 0.9547 - Accuracy: 0.9645 - val_CELoss: 0.2577 - val_F1_Score: 0.9001 - val_Accuracy: 0.8993\n",
      "Fold: 1 Epoch: 21/50 Max F1: 0.9002408344852587\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.0977 - F1_Score: 0.9555 - Accuracy: 0.9598 - val_CELoss: 0.2202 - val_F1_Score: 0.9096 - val_Accuracy: 0.9115\n",
      "Fold: 1 Epoch: 22/50 Max F1: 0.9095808734521885\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.1089 - F1_Score: 0.9599 - Accuracy: 0.9645 - val_CELoss: 0.2832 - val_F1_Score: 0.8848 - val_Accuracy: 0.8929\n",
      "Fold: 1 Epoch: 23/50 Max F1: 0.9095808734521885\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.1034 - F1_Score: 0.9470 - Accuracy: 0.9654 - val_CELoss: 0.2070 - val_F1_Score: 0.9110 - val_Accuracy: 0.9242\n",
      "Fold: 1 Epoch: 24/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 182s 8s/step - CELoss: 0.0964 - F1_Score: 0.9575 - Accuracy: 0.9697 - val_CELoss: 0.3158 - val_F1_Score: 0.8638 - val_Accuracy: 0.9034\n",
      "Fold: 1 Epoch: 25/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.1065 - F1_Score: 0.9384 - Accuracy: 0.9555 - val_CELoss: 0.1777 - val_F1_Score: 0.8830 - val_Accuracy: 0.9115\n",
      "Fold: 1 Epoch: 26/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.1178 - F1_Score: 0.9539 - Accuracy: 0.9616 - val_CELoss: 0.3182 - val_F1_Score: 0.8618 - val_Accuracy: 0.8993\n",
      "Fold: 1 Epoch: 27/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.1180 - F1_Score: 0.9628 - Accuracy: 0.9593 - val_CELoss: 0.2926 - val_F1_Score: 0.9046 - val_Accuracy: 0.8970\n",
      "Fold: 1 Epoch: 28/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.1557 - F1_Score: 0.9377 - Accuracy: 0.9474 - val_CELoss: 0.2995 - val_F1_Score: 0.8060 - val_Accuracy: 0.8733\n",
      "Fold: 1 Epoch: 29/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 184s 8s/step - CELoss: 0.1471 - F1_Score: 0.9296 - Accuracy: 0.9489 - val_CELoss: 0.2830 - val_F1_Score: 0.9001 - val_Accuracy: 0.8981\n",
      "Fold: 1 Epoch: 30/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 183s 8s/step - CELoss: 0.1345 - F1_Score: 0.9206 - Accuracy: 0.9384 - val_CELoss: 0.3191 - val_F1_Score: 0.8970 - val_Accuracy: 0.8981\n",
      "Fold: 1 Epoch: 31/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 187s 8s/step - CELoss: 0.1583 - F1_Score: 0.9135 - Accuracy: 0.9512 - val_CELoss: 0.2851 - val_F1_Score: 0.8666 - val_Accuracy: 0.8993\n",
      "Fold: 1 Epoch: 32/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 186s 8s/step - CELoss: 0.2662 - F1_Score: 0.8901 - Accuracy: 0.9058 - val_CELoss: 0.3397 - val_F1_Score: 0.8283 - val_Accuracy: 0.8547\n",
      "Fold: 1 Epoch: 33/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 186s 8s/step - CELoss: 0.1610 - F1_Score: 0.9155 - Accuracy: 0.9318 - val_CELoss: 0.3033 - val_F1_Score: 0.8532 - val_Accuracy: 0.9115\n",
      "Fold: 1 Epoch: 34/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 185s 8s/step - CELoss: 0.1312 - F1_Score: 0.9110 - Accuracy: 0.9498 - val_CELoss: 0.3332 - val_F1_Score: 0.8920 - val_Accuracy: 0.8929\n",
      "Fold: 1 Epoch: 35/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 186s 8s/step - CELoss: 0.1410 - F1_Score: 0.9050 - Accuracy: 0.9290 - val_CELoss: 0.2362 - val_F1_Score: 0.8537 - val_Accuracy: 0.9126\n",
      "Fold: 1 Epoch: 36/50 Max F1: 0.9109750698325967\n",
      "23/23 [===============] - 188s 8s/step - CELoss: 0.1238 - F1_Score: 0.9194 - Accuracy: 0.9503 - val_CELoss: 0.2503 - val_F1_Score: 0.9210 - val_Accuracy: 0.9138\n",
      "Fold: 1 Epoch: 37/50 Max F1: 0.9209771287141977\n",
      "23/23 [===============] - 188s 8s/step - CELoss: 0.0922 - F1_Score: 0.9423 - Accuracy: 0.9631 - val_CELoss: 0.2604 - val_F1_Score: 0.8603 - val_Accuracy: 0.8837\n",
      "Fold: 1 Epoch: 38/50 Max F1: 0.9209771287141977\n",
      "14/23 [========>......] - ETA: 1:10 - CELoss: 0.0816 - F1_Score: 0.9609 - Accuracy: 0.9710"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(s):\n",
    "    \n",
    "    model = VGG16BN()\n",
    "    model.to(device)\n",
    "    \n",
    "    celoss = nn.CrossEntropyLoss(weight=torch.tensor([6.0241, 3.6496, 2.4390, 1.0823, 4.3860]).to(0, dtype=torch.float))\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    \n",
    "    max_f1 = 0\n",
    "    \n",
    "    train = TrashDataset(metadata.iloc[train_idx,:], ROOT, train_transform)\n",
    "    test = TrashDataset(metadata.iloc[test_idx,:], ROOT, val_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4) \n",
    "    test_loader = torch.utils.data.DataLoader(test, \n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Fold: {fold+1} Epoch: {epoch+1}/{EPOCHS} Max F1: {max_f1}')\n",
    "        pbar = pkbar.Kbar(target=len(train_loader), width=15)\n",
    "        # Training \n",
    "        model.train()\n",
    "        for batch_num, inputs in enumerate(train_loader):\n",
    "            images = inputs['image'].to(0, dtype=torch.float)\n",
    "            labels = inputs['label'].to(0, dtype=torch.long)\n",
    "\n",
    "            # Forward Feeding\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss_value = celoss(outputs, labels)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Generate Metrics and Update Progress Bar Every 10~20 Batches\n",
    "            predictions = torch.max(outputs, 1)[1].cpu().detach().numpy()\n",
    "            metric_label = labels.cpu().detach().numpy()\n",
    "            f1 = f1_score(metric_label, predictions, average='macro')\n",
    "            accuracy = accuracy_score(metric_label, predictions)\n",
    "\n",
    "            # Update Progress Bar\n",
    "            pbar.update(batch_num, values=[('CELoss', loss_value.item()), ('F1_Score', f1),\n",
    "                                           ('Accuracy', accuracy)])\n",
    "\n",
    "            # Free up CUDA memory\n",
    "            del images, labels\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        val_loss, val_f1, val_acc = [], [], []\n",
    "        model.eval()\n",
    "        for inputs in test_loader:\n",
    "            images = inputs['image'].to(0, dtype=torch.float)\n",
    "            labels = inputs['label'].to(0, dtype=torch.long)\n",
    "\n",
    "            # Forward Feeding\n",
    "            outputs = model(images)\n",
    "            predictions = torch.max(outputs, 1)[1].cpu().detach().numpy()\n",
    "            metric_label = labels.cpu().detach().numpy()\n",
    "\n",
    "            # Metric Calculation\n",
    "            val_loss.append(celoss(outputs, labels).item())\n",
    "            val_f1.append(f1_score(metric_label, predictions, average='macro'))\n",
    "            val_acc.append(accuracy_score(metric_label, predictions))\n",
    "\n",
    "        pbar.add(1, values=[('val_CELoss', sum(val_loss)/len(val_loss)),\n",
    "                            ('val_F1_Score', sum(val_f1)/len(val_f1)),\n",
    "                            ('val_Accuracy', sum(val_acc)/len(val_acc))])\n",
    "        if sum(val_f1)/len(val_f1) > max_f1:\n",
    "            max_f1 = sum(val_f1)/len(val_f1)\n",
    "            torch.save(model.state_dict(), f'../asun/Smart-Trash/models/kfold-nometa-densenet169/model{fold}-{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18cff5-bbcf-46d8-934b-af1c5d03922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validation(model, validation_loader, loss, device):\n",
    "#     model = model.to(0)\n",
    "#     model.eval()\n",
    "#     loss_log, acc_log, f1_log = [], [], []\n",
    "#     for batch_num, inputs in enumerate(validation_loader):\n",
    "#         # Load data onto device: GPU or CPU\n",
    "#         images = inputs['image'].to(0, dtype=torch.float)\n",
    "#         labels = inputs['label'].to(0, dtype=torch.long)\n",
    "        \n",
    "#         # Forward Feeding\n",
    "#         outputs = model(images)\n",
    "#         loss_value = loss(outputs, labels).mean()\n",
    "#         preds = torch.max(outputs, 1)[1].cpu().detach().numpy()\n",
    "#         loss_log.append(loss_value)\n",
    "        \n",
    "#         # Metric Calculation\n",
    "#         acc = accuracy_score(preds, labels.cpu().detach().numpy())\n",
    "#         f1 = f1_score(labels.cpu().detach().numpy(), preds, average='macro')\n",
    "#         acc_log.append(acc)\n",
    "#         f1_log.append(f1)\n",
    "#         # Free up memory\n",
    "#         del images, labels\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#     return sum(loss_log) / len(loss_log), sum(acc_log) / len(acc_log), sum(f1_log) / len(f1_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98990e20-2e7e-456b-8f71-535c5981b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epochs, model, batchsize, train_loader, validation_loader, \n",
    "#           loss, optimizer, device):\n",
    "#     model = model.to(0)\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         print(f'Epochs: {epoch+1}/{EPOCHS}')\n",
    "#         kbar = pkbar.Kbar(target=len(train_loader), width=10)\n",
    "#         for batch_num, inputs in enumerate(train_loader):\n",
    "#             # Load data onto device: GPU or CPU\n",
    "#             images = inputs['image'].to(0, dtype=torch.float)\n",
    "#             labels = inputs['label'].to(0, dtype=torch.long)\n",
    "#             # Zero the optimizer\n",
    "#             optimizer.zero_grad()\n",
    "#             # Forward Feeding\n",
    "#             outputs = model(images)\n",
    "#             loss_value = loss(outputs, labels)\n",
    "#             # Backpropagation\n",
    "#             loss_value.mean().backward()\n",
    "#             optimizer.step()\n",
    "#             # Metric Calculation\n",
    "#             preds = torch.max(outputs, 1)[1].cpu().detach().numpy()\n",
    "#             acc = accuracy_score(preds, labels.cpu().detach().numpy())\n",
    "#             train_f1 = f1_score(labels.cpu().detach().numpy(), preds, average='macro')\n",
    "#             # Update progress bar\n",
    "#             kbar.update((batch_num+1), values=[('loss', loss_value), \n",
    "#                                                ('acc', acc), \n",
    "#                                                ('f1', train_f1)])\n",
    "#             # Free up memory\n",
    "#             del images, labels\n",
    "#             torch.cuda.empty_cache()\n",
    "#         l, ac, f1 = validation(model, validation_loader, loss, device)\n",
    "#         kbar.add(1, values=[('val_loss', l), \n",
    "#                             ('val_acc', ac),\n",
    "#                             ('val_f1', f1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2d752-b135-4f8b-8d87-dcb88b04e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.083\n",
    "# model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "# train(EPOCHS, model, BATCH_SIZE, train_loader, valid_loader,\n",
    "#       celoss, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b0bda-95e4-481c-b939-d1da01cf27a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "_number": 0.5748044936240306,
   "display_name": "Python",
   "kernel_gpu_num": "1",
   "kernel_language_version": "3.7",
   "kernel_pytorch_version": "1.4.0",
   "kernel_tf_version": "2.1.0",
   "kernel_train_type": "gpu",
   "language": "Python",
   "name": "python_universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
