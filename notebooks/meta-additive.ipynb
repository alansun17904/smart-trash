{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb7a748-2e95-4364-bbec-7de6cfc261c0",
   "metadata": {},
   "source": [
    "# Baseline Transfer Learning Model for TrashNet Classification\n",
    "Our baseline model will include a pretrained DenseNet feature extractor with a shallow and wide CNN head. This model will have a homogenous learning rate. We are going to use K-Fold CV as well as F1 score and multi-class AUC to validate our model.\n",
    "This model acts as a stepping stone / template for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff32728-4b74-4808-a272-6ee2de3113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import pkbar\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50488451-6256-42e9-971e-3dddc98aae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT = Path('../asun/Smart-Trash/data/isbnet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78757c-1474-4ec2-8227-b03bc848ef13",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "For the baseline model, we will not be applying any data augmentation or color manipulation.\n",
    "- Get the index CSV file that includes all files their respective directory and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1923b17-077b-4ed5-8c86-ac0164e8c703",
   "metadata": {},
   "source": [
    "### Trash Dataset\n",
    "Dataset object to handle various sets of data that we will be dealing with including: TrashNet, ISBNet, and ISBNet extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e076ac-98e0-4ee1-a6c8-202652bc7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(length, split, shuffle=True):\n",
    "    \"\"\"\n",
    "    :returns: random samplers for both the training dataset and the validation dataset.\n",
    "    \"\"\"\n",
    "    indicies = list(range(length))\n",
    "    split = int(np.floor(split * length))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indicies)\n",
    "    \n",
    "    train_indicies, val_indicies = indicies[split:], indicies[:split]\n",
    "    train_sampler = data.SubsetRandomSampler(train_indicies)\n",
    "    val_sampler = data.SubsetRandomSampler(val_indicies)\n",
    "    \n",
    "    return train_sampler, val_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012fe5a-2d08-4e9f-8525-c3de980ffccf",
   "metadata": {},
   "source": [
    "A split function is defined to split the dataset after it is defined as a `DataSet` object. This makes it really easy to handle, because after splitting it, we are creating a sampler object. There is no need to modify the dataset object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd31f769-9237-436f-8823-3548e4c45a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashDataset(Dataset):\n",
    "    def __init__(self, metadata: pd.DataFrame, directory: Path, land_dict: dict, noland=0., transform=None):\n",
    "        \"\"\"\n",
    "        metadata: DataFrame that contains information about each image and their labels.\n",
    "        directory: the directory where the trash data is kept\n",
    "        root_dir: path to the `directory`\n",
    "        transform: optional augmentations that are to be applied onto the images\n",
    "        \"\"\"\n",
    "        self.images_folder = directory\n",
    "        self.meta = metadata\n",
    "        self.transform = transform\n",
    "        self.label_dict = {\n",
    "            'cans': 0,\n",
    "            'landfill': 1,\n",
    "            'paper': 2,\n",
    "            'plastic': 3,\n",
    "            'tetrapak': 4\n",
    "        }\n",
    "        self.noland = noland\n",
    "        self.land_dict = land_dict\n",
    "  \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.meta)\n",
    "  \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        image = Image.open(self.meta.iloc[idx, 0])\n",
    "        \n",
    "        # Labels preprocessing\n",
    "        labels = [0] * 5\n",
    "        labels[self.label_dict[self.meta.iloc[idx, 1]]] = 1\n",
    "        \n",
    "        # Metadata preprocessing\n",
    "        trashcan_time = self._label_smoothing(np.array(self.meta.iloc[idx, 4]), 0.2)\n",
    "        landmarks = self._encode(self.meta.iloc[idx, 6], self.land_dict, padding=self.noland)\n",
    "        trashcan_location = self.meta.iloc[idx, 5]\n",
    "        distances = [self._pythag(v) for v in trashcan_location]\n",
    "        for index, item in enumerate(self.meta.iloc[idx, 6]):\n",
    "            landmarks[self.land_dict[item]] = distances[index]\n",
    "        \n",
    "        \n",
    "        sample = {'image': image,\n",
    "                  'meta': torch.tensor(np.concatenate((trashcan_time,\n",
    "                                                      landmarks), axis=0), dtype=torch.float),\n",
    "                  'path': self.meta.iloc[idx, 0],\n",
    "                  'label': torch.tensor(self.label_dict[self.meta.iloc[idx, 1]], dtype=torch.float)}\n",
    "\n",
    "        if self.transform:\n",
    "              sample['image'] = self.transform(sample['image'])\n",
    "        return sample\n",
    "    \n",
    "    def _encode(self, need_encoding: list, reference: dict, padding=0.) -> np.array:\n",
    "        one_hot = np.zeros(len(reference))\n",
    "        for item in need_encoding:\n",
    "            one_hot[reference[item]] = 1.\n",
    "        one_hot = one_hot / self._pythag(one_hot)\n",
    "        one_hot = np.where(one_hot==0, padding, one_hot)\n",
    "        return -np.log(one_hot)\n",
    "    \n",
    "    def _label_smoothing(self, labels: np.array, smoothing: float) -> np.array:\n",
    "        return np.where(labels==1, 1-smoothing, 0+smoothing)\n",
    "    \n",
    "    def _pythag(self, coor):\n",
    "        return np.linalg.norm(coor, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb85a6f-d2b0-4f4c-a841-83c073cfcee5",
   "metadata": {},
   "source": [
    "Because we are using cross entropy loss we can express the loss as simply a scalar. This scalar is between [0-numclasses]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeab94d-cdef-4121-9f0a-c328bdc1da4e",
   "metadata": {},
   "source": [
    "# Model and Training Setup\n",
    "- VGG16 pretrained with ImageNet\n",
    "- FC layers as classifiers. I did not use log-softmax activation or a normalization on the last layer because predictions are based on max value. A normalizing activation function on the last layer would diminish the network's ability to learn.\n",
    "- CrossEntropy loss and Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73201478-2760-4bf1-9a0b-982362734056",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c2807d-ce6b-42b6-b61c-fcac2935e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.head = models.resnet50(pretrained=True)\n",
    "        # Remove classification layers so that we are able to add our own CNN layers\n",
    "        self.head.requires_grad = False\n",
    "        self.head.fc = nn.Sequential(\n",
    "                                nn.Linear(2048, 1024, bias=True),\n",
    "                                nn.BatchNorm1d(1024),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.25))\n",
    "        self.fc1 = nn.Sequential(\n",
    "                                nn.Linear(1024, 512, bias=True),\n",
    "                                nn.BatchNorm1d(512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.50),\n",
    "                                nn.Linear(512, 5, bias=True),\n",
    "                                nn.ReLU()\n",
    "        )\n",
    "        self.meta_fc1 = nn.Linear(21, 16, bias=True)\n",
    "        self.meta_relu1 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, image, metadata):\n",
    "        metadata = self.meta_fc1(metadata)\n",
    "        metadata = self.meta_relu1(metadata)\n",
    "        image = self.head(image)\n",
    "        image = self.fc1(image)\n",
    "        return image + metadata\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"\n",
    "        https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "        \"\"\"\n",
    "        size = x.size()[1:]  # get all dimensions except for batch size\n",
    "        features = 1\n",
    "        for s in size:\n",
    "            features *= s\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f91cc5-4a72-4929-b9b4-c7e9e4760f21",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033210d6-ced3-4aef-bb8a-38afa8a8572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463379eb-e664-4ad5-991d-c6ff7b485078",
   "metadata": {},
   "source": [
    "### KFold Training and CV\n",
    "* KFold setup with `StratifiedKFold`\n",
    "* Creating Dataloaders in training loop.\n",
    "* Using Adam and CrossEntropy Loss\n",
    "* Center crop on images to make them 224x224 so ResNet will be able to take them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f4de05-c6b6-46f9-9bfb-7683242b02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.RandomAffine(15),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f05702-0e98-4c61-8b33-ee0fc3e2a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(ROOT / 'metadata.csv')\n",
    "land_dict = json.load(open(ROOT / 'metalabels.json'))['landmarks']\n",
    "metadata['categorial_time'] = metadata['categorial_time'].apply(ast.literal_eval)\n",
    "metadata['trashcan_time'] = metadata['trashcan_time'].apply(ast.literal_eval)\n",
    "metadata['trashcan_location'] = metadata['trashcan_location'].apply(ast.literal_eval)\n",
    "metadata['landmarks'] = metadata['landmarks'].apply(ast.literal_eval)\n",
    "TRAIN_VAL = TrashDataset(metadata, ROOT, land_dict=land_dict, noland=1e3, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ea494-5163-438e-9eef-334957b03e26",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1286a7-edcf-4916-b032-1a24e895e827",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98354e86-79c7-4884-8410-2bca15a21410",
   "metadata": {},
   "outputs": [],
   "source": [
    "trash_labels = metadata.iloc[:,1].values\n",
    "s = StratifiedKFold(n_splits=FOLDS, shuffle=True).split(metadata, trash_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b30cd-ba64-430d-8545-649bce975fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 \t Epoch 1/50 \t Max F1: 0\n",
      "23/23 [===============] - 330s 14s/step - CELoss: 1.4375 - F1_Score: 0.3524 - Accuracy: 0.4105 - val_CELoss: 1.3444 - val_F1_Score: 0.4094 - val_Accuracy: 0.5023\n",
      "Fold: 1 \t Epoch 2/50 \t Max F1: 0.4094\n",
      "22/23 [=============>.] - ETA: 15s - CELoss: 1.0404 - F1_Score: 0.5733 - Accuracy: 0.6605"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(s):\n",
    "    max_f1 = 0\n",
    "    \n",
    "    model = ResNet()\n",
    "    model.to(0)\n",
    "    \n",
    "    celoss = nn.CrossEntropyLoss(weight=torch.tensor([6.0241, 3.6496, 2.4390, 1.0823, 4.3860]).to(0, dtype=torch.float))\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4,\n",
    "                       weight_decay=1e-9)\n",
    "    \n",
    "    train = TrashDataset(metadata.iloc[train_idx,:], ROOT, land_dict, noland=1e3, transform=train_transform)\n",
    "    test = TrashDataset(metadata.iloc[test_idx,:], ROOT, land_dict, noland=1e3, transform=val_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4) \n",
    "    valid_loader = torch.utils.data.DataLoader(test, \n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Fold: {fold+1} \\t Epoch {epoch+1}/{EPOCHS} \\t Max F1: {round(max_f1,4)}')\n",
    "        pbar = pkbar.Kbar(target=len(train_loader), width=15)\n",
    "        # Training \n",
    "        model.train()\n",
    "        for batch_num, inputs in enumerate(train_loader):\n",
    "            images = inputs['image'].to(0, dtype=torch.float)\n",
    "            _meta = inputs['meta'].to(0, dtype=torch.float)\n",
    "            labels = inputs['label'].to(0, dtype=torch.long)\n",
    "\n",
    "            # Forward Feeding\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, _meta)\n",
    "            loss_value = celoss(outputs, labels)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Generate Metrics and Update Progress Bar Every 10~20 Batches\n",
    "            predictions = torch.max(outputs, 1)[1].cpu().detach().numpy()\n",
    "            metric_label = labels.cpu().detach().numpy()\n",
    "            f1 = f1_score(metric_label, predictions, average='macro')\n",
    "            accuracy = accuracy_score(metric_label, predictions)\n",
    "\n",
    "            # Update Progress Bar\n",
    "            pbar.update(batch_num, values=[('CELoss', loss_value.item()), ('F1_Score', f1),\n",
    "                                           ('Accuracy', accuracy)])\n",
    "\n",
    "            # Free up CUDA memory\n",
    "            del labels\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        val_loss, val_f1, val_acc = [], [], []\n",
    "        model.eval()\n",
    "        for inputs in valid_loader:\n",
    "            images = inputs['image'].to(0, dtype=torch.float)\n",
    "            labels = inputs['label'].to(0, dtype=torch.long)\n",
    "            _meta = inputs['meta'].to(0, dtype=torch.float)\n",
    "\n",
    "            # Forward Feeding\n",
    "            outputs = model(images, _meta)\n",
    "            predictions = torch.max(outputs, 1)[1].cpu().detach().numpy()\n",
    "            metric_label = labels.cpu().detach().numpy()\n",
    "\n",
    "            # Metric Calculation\n",
    "            val_loss.append(celoss(outputs, labels).item())\n",
    "            val_f1.append(f1_score(metric_label, predictions, average='macro'))\n",
    "            val_acc.append(accuracy_score(metric_label, predictions))\n",
    "\n",
    "        pbar.add(1, values=[('val_CELoss', sum(val_loss)/len(val_loss)),\n",
    "                            ('val_F1_Score', sum(val_f1)/len(val_f1)),\n",
    "                            ('val_Accuracy', sum(val_acc)/len(val_acc))])\n",
    "        if sum(val_f1)/len(val_f1) > max_f1:\n",
    "            max_f1 = sum(val_f1)/len(val_f1)\n",
    "            torch.save(model.state_dict(), f'../asun/Smart-Trash/models/kfold-resnet-meta-additive/model{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9f97e-05ea-496d-973b-dbd842435769",
   "metadata": {},
   "source": [
    "# Inference and Model EDA\n",
    "- Load weights from model with best validation loss score.\n",
    "- Predict on all samples in \"pure\" isbnet\n",
    "- Generate a confusion matrix of these samples and display false positives / negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a594968-57b5-49d1-bfca-cee778d63314",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ResNet()\n",
    "inference = inference.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a76e87-9214-41cb-945e-8e14a7472323",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.load_state_dict(torch.load('../asun/Smart-Trash/models/resnet-meta-baseline/model109.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc15f4-103f-4237-94cc-e57c288a6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.eval()\n",
    "testset = TrashDataset(metadata, ROOT, transform)\n",
    "testset = DataLoader(testset, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58afd253-4a27-48c8-829e-d7da52d64fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = np.array([])\n",
    "labels = np.array([])\n",
    "predictions = np.array([])\n",
    "\n",
    "for index, sample in enumerate(testset):\n",
    "    image = sample['meta'].to(0)\n",
    "    labels = np.append(labels, sample['label'])\n",
    "    filenames = np.append(filenames, sample['path'])\n",
    "    predict = torch.max(inference(image), 1)[1].cpu().cpu().detach().numpy()\n",
    "    predictions = np.append(predictions, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235c05f-3790-4fc8-aa5e-f6cdbb2727e0",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862bc07-9b6d-4975-98f0-1920474a35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "c_matrix = confusion_matrix(labels, predictions)\n",
    "f, ax = plt.subplots(figsize=(15, 13))\n",
    "\n",
    "# Cell Labels\n",
    "cell_labels = c_matrix.flatten()\n",
    "\n",
    "# Categories\n",
    "categories = ['Cans', 'Landfill', 'Paper', 'Plastic', 'Tetrapak']\n",
    "\n",
    "# Color Scheme\n",
    "cmap = sns.diverging_palette(160, 250, s=100, l=70, n=10, as_cmap=True)\n",
    "\n",
    "#  Metrics\n",
    "f1 = f1_score(labels, predictions, average='macro')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "plt.title('Confusion Matrix of ResNet50 Baseline Predictions without Metadata', fontsize=20)\n",
    "sns.heatmap(c_matrix, fmt='', annot=c_matrix, cbar_kws={\"shrink\": .5},\n",
    "            xticklabels=categories, yticklabels=categories, linewidth=1.5, square=True)\n",
    "plt.xlabel('Predictions Labels\\nAccuracy: {:4f}, F1 Score: {:4f}'.format(accuracy, f1), fontsize=18)\n",
    "plt.ylabel('Actual Labels', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7aa5f-a40e-49ce-a11a-7dd5f3f60816",
   "metadata": {},
   "source": [
    "## Sample Analysis\n",
    "In this section, we examine the individual samples where the model predicted correctly and incorrectly. We first define a helper function to handle displaying images in groups of grids.\n",
    "\n",
    "**Helper functions and helper map defined below.**\n",
    "- `labels_dict` --> Map that maps indicices to their respective labels.\n",
    "- `imshow` --> Displays a list of images\n",
    "- `get_images` --> Gets encoded image arrays from dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0195ba-f1d0-49fc-95a7-9235e584ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    0: 'Can',\n",
    "    1: 'Landfill', \n",
    "    2: 'Paper',\n",
    "    3: 'Plastic',\n",
    "    4: 'Tetrapak'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172902f7-1127-4232-a8a2-d30a726a66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None, xlabel=None):\n",
    "    \"\"\"\n",
    "    Displays a tensor of tensor images.\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(20, 15))\n",
    "    ax.grid(False)\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize='20')\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel('Predictions: ' + str(xlabel), fontsize='18')\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942a511-4aba-49d5-86cb-be7dd5696d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(loader, idxs, num):\n",
    "    \"\"\"\n",
    "    Gets images from the `dataset` object by indexing them using the `num` parameters.\n",
    "    \"\"\"\n",
    "    if len(idxs) < num:\n",
    "        if len(idxs) == 0:\n",
    "            return\n",
    "        return torch.tensor([loader[i]['image'].numpy() for i in idxs], dtype=torch.float)\n",
    "    else:\n",
    "        images = []\n",
    "        count = 0\n",
    "        for idx in idxs:\n",
    "            images.append(loader[idx]['image'].numpy())\n",
    "            if count + 1 == num:\n",
    "                return torch.tensor(images, dtype=torch.float)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d7e22-54a7-465e-99ca-2b343bcf34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(labels_dict, labels, predictions, category, loader, correct=True, nums=4):\n",
    "    c_labels = np.where(labels==category)\n",
    "    c_predictions = np.where(predictions==category)\n",
    "    \n",
    "    # Get sample indicies\n",
    "    if correct:\n",
    "        idxs = np.intersect1d(c_labels, c_predictions)\n",
    "    else:\n",
    "        idxs = np.setdiff1d(c_labels, c_predictions)\n",
    "    \n",
    "    images = get_images(loader, idxs, nums)\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    if len(idxs) < nums:\n",
    "        x_label = [labels_dict[int(predictions[i])] for i in idxs[0:len(idxs)]]\n",
    "    else:\n",
    "        x_label = [labels_dict[int(predictions[i])] for i in idxs[0:nums]]\n",
    "    \n",
    "    s = 'Correctly' if correct else 'Incorrectly'\n",
    "    imshow(grid, title=f'{s} Predicted Pictures ({labels_dict[category]} Class)', xlabel=x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ad57b-269c-4212-88ae-9301f0ceb520",
   "metadata": {},
   "source": [
    "## Cans Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe9dda-16fd-45a5-b7b6-c8dcf7748bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 0, inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e79045-edd7-489d-ae86-a5f5d6eb95ab",
   "metadata": {},
   "source": [
    "These are pictures of cans that were correctly predicted by the classifier. The `RandomResizedCrop` randomly changes the scale, this causes the large variation in validation scores across epochs. When the validation score is really high that means it has discovered a crop that is really beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ab948-0298-4fa1-92c7-a11660da7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 0, correct=False, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd015d4-5d6e-453e-abfa-ce388e41de4e",
   "metadata": {},
   "source": [
    "## Landfill Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271fae1-e20b-4f0b-8676-373fca242542",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 1, correct=True, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bffd4-7968-495c-86d6-a235d08a9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 1, correct=False, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ca344-1191-4a3f-a1e4-6e2d9a43d2f2",
   "metadata": {},
   "source": [
    "## Paper Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded93b0a-e544-4a9f-a07e-cc20d35cf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 2, correct=True, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c92fc-2203-4b79-912a-a50b73a31a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 2, correct=False, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f71d0e-776d-487b-814c-058484eb2c67",
   "metadata": {},
   "source": [
    "## Plastics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e1742-7be4-4903-8e39-855d32035320",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 3, correct=True, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc7138-5c38-4416-a815-07221698376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 3, correct=False, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f016002-01d6-4c11-9480-f54312be7b4f",
   "metadata": {},
   "source": [
    "## Tetrapak Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbfb78-bd3a-4974-aec5-9a16b7fa4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 4, correct=True, loader=inference_loader, nums=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00e999-572d-464c-8644-884122ec74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(labels_dict, labels, predictions, 4, correct=False, loader=inference_loader, nums=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "_number": 0.5748044936240306,
   "display_name": "Python",
   "kernel_gpu_num": "1",
   "kernel_language_version": "3.7",
   "kernel_pytorch_version": "1.4.0",
   "kernel_tf_version": "2.1.0",
   "kernel_train_type": "gpu",
   "language": "Python",
   "name": "python_universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
